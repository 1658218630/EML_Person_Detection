{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5957f000-626f-461b-8349-42cf13c69225",
   "metadata": {},
   "source": [
    "# Load Vision Model\n",
    "\n",
    "First we load in the data and our model into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc8843-8317-42cb-8df8-f73ce05c8c26",
   "metadata": {},
   "source": [
    "**Important:** For this notebook to function it needs to be execuetd with `export LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libgomp.so.1:/$LD_PRELOAD` else you will get a error. \n",
    "I included this in jupyter by adding a docker environment variable: `-e LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libgomp.so.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d608437-5301-4afc-b0f6-38940f772c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4fe20b9-18e5-498c-a48e-b0adab501705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from utils.dataloader import VOCDataLoader\n",
    "loader = VOCDataLoader(train=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aadf5a56-5b7e-471d-a20f-8877c8da133c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyYoloV2(\n",
       "  (pad): ReflectionPad2d((0, 1, 0, 1))\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv7): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn8): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv9): Conv2d(1024, 125, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tinyyolov2 import TinyYoloV2\n",
    "from utils.yolo import nms, filter_boxes\n",
    "from utils.viz import display_result\n",
    "\n",
    "# make an instance with 20 classes as output\n",
    "net = TinyYoloV2(num_classes=20)\n",
    "\n",
    "# load pretrained weights\n",
    "sd = torch.load(\"voc_pretrained.pt\")\n",
    "net.load_state_dict(sd)\n",
    "\n",
    "#put network in evaluation mode\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb9c991-2879-4cfc-9cba-fd2b2c872656",
   "metadata": {},
   "source": [
    "# Define Camera Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2717d23b-f432-4990-9830-b9c981ddcbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.camera import CameraDisplay\n",
    "import time\n",
    "import cv2\n",
    "now = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d45469fe-b164-4633-b3d4-78d21612bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(image):\n",
    "    # Ensure the input tensor is correct\n",
    "    if image.shape != (1, 3, 320, 320):\n",
    "        raise ValueError(f\"Invalid input shape: {image.shape}, expected (1, 3, 320, 320)\")\n",
    "\n",
    "    # Pass through the network\n",
    "    with torch.no_grad():\n",
    "        output = net(image)\n",
    "\n",
    "    # Process YOLO output\n",
    "    output = filter_boxes(output, 0.001)\n",
    "    output = nms(output, 0.1)\n",
    "\n",
    "    # Convert the tensor back to a NumPy array for OpenCV\n",
    "    image_np = image.squeeze(0).permute(1, 2, 0).numpy()\n",
    "    image_np = (image_np * 255).astype(np.uint8)  # Scale back to 0-255 and convert to uint\n",
    "\n",
    "    # Visualize the results if there are any detections\n",
    "    img_shape = 320 \n",
    "    if output:\n",
    "        bboxes = torch.stack(output, dim=0)\n",
    "        for i in range(bboxes.shape[1]):\n",
    "            if bboxes[0, i, -1] >= 0:\n",
    "                # Debug\n",
    "                print(f\"Detection detected: class {num_to_class(int(bboxes[0, i, 5]))}, confidence {bboxes[0, i, 4]:.2f}\")\n",
    "                \n",
    "                # Calculate bounding box coordinates\n",
    "                cx = int(bboxes[0, i, 0] * img_shape - bboxes[0, i, 2] * img_shape / 2)\n",
    "                cy = int(bboxes[0, i, 1] * img_shape - bboxes[0, i, 3] * img_shape / 2)\n",
    "                w = int(bboxes[0, i, 2] * img_shape)\n",
    "                h = int(bboxes[0, i, 3] * img_shape)\n",
    "    \n",
    "                # Draw rectangle on the image\n",
    "                cv2.rectangle(\n",
    "                    image,  # OpenCV image (numpy array)\n",
    "                    (cx, cy),  # Top-left corner\n",
    "                    (cx + w, cy + h),  # Bottom-right corner\n",
    "                    (0, 0, 255),  # Color (BGR) - Red in this case\n",
    "                    2  # Thickness\n",
    "                )\n",
    "    \n",
    "                # Add label text\n",
    "                label = f\"{num_to_class(int(bboxes[0, i, 5]))} {bboxes[0, i, 4]:.2f}\"\n",
    "                cv2.putText(\n",
    "                    image,\n",
    "                    label,\n",
    "                    (cx, cy - 10),  # Slightly above the rectangle\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,  # Font\n",
    "                    0.5,  # Font scale\n",
    "                    (0, 0, 255),  # Color (BGR) - Red\n",
    "                    1  # Thickness\n",
    "                )\n",
    "                \n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2be5c5f-2862-4e1c-b4fa-0de8c6849d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback function (your detection pipeline)\n",
    "# Make sure to first load all your pipeline code and only at the end init the camera\n",
    "\n",
    "def callback(image):\n",
    "    global now\n",
    "\n",
    "    if image is None:\n",
    "        raise ValueError(\"Received empty frame from the camera.\")\n",
    "\n",
    "    # Resize and preprocess the image\n",
    "    img_resized = cv2.resize(image, (320, 320))\n",
    "    img_tensor = torch.tensor(img_resized, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "\n",
    "    # Predict and visualize\n",
    "    img_with_predictions = get_predictions(img_tensor)\n",
    "\n",
    "    # Ensure the image is a NumPy array for OpenCV\n",
    "    img_with_predictions = img_with_predictions.numpy() if isinstance(img_with_predictions, torch.Tensor) else img_with_predictions\n",
    "\n",
    "    # Add FPS to the image\n",
    "    fps = f\"{int(1 / (time.time() - now))}\"\n",
    "    now = time.time()\n",
    "    cv2.putText(img_with_predictions, f\"fps={fps}\", (2, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    return img_with_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "235e8da7-9935-4843-bac0-aa41a66e2153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing camera...\n",
      "GST_ARGUS: Creating output stream\n",
      "CONSUMER: Waiting until producer is connected...\n",
      "GST_ARGUS: Available Sensor modes :\n",
      "GST_ARGUS: 3264 x 2464 FR = 21.000000 fps Duration = 47619048 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 3264 x 1848 FR = 28.000001 fps Duration = 35714284 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1920 x 1080 FR = 29.999999 fps Duration = 33333334 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1640 x 1232 FR = 29.999999 fps Duration = 33333334 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1280 x 720 FR = 59.999999 fps Duration = 16666667 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1280 x 720 FR = 120.000005 fps Duration = 8333333 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: Running with following settings:\n",
      "   Camera index = 0 \n",
      "   Camera mode  = 5 \n",
      "   Output Stream W = 1280 H = 720 \n",
      "   seconds to Run    = 0 \n",
      "   Frame Rate = 120.000005 \n",
      "GST_ARGUS: Setup Complete, Starting captures for 0 seconds\n",
      "GST_ARGUS: Starting repeat capture requests.\n",
      "CONSUMER: Producer has connected; continuing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1.439] global cap_gstreamer.cpp:1777 open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c1974c9ce949a68bf57941dda656cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the camera with the callback\n",
    "cam = CameraDisplay(callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb687c4-8ee6-4ec8-b496-547954b0b3bc",
   "metadata": {},
   "source": [
    "# Camera Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "602c204d-bf7e-4e28-a4a9-787d6f738996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# The camera stream can be started with cam.start()\n",
    "# The callback gets asynchronously called (can be stopped with cam.stop())\n",
    "cam.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8a0630-2ca5-4055-8d7d-1a12259afc48",
   "metadata": {},
   "source": [
    "Execute below, to stop camera loop.\n",
    "\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7b5d0a-a74b-4537-a76e-22fa16be1175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera released\n"
     ]
    }
   ],
   "source": [
    "# The camera should always be stopped and released for a new camera is instantiated (calling CameraDisplay(callback) again)\n",
    "cam.stop()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659cc34-3bf8-4b3a-9e60-675eb95c68b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
